from deep_translator import GoogleTranslator, MyMemoryTranslator
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from search_link import *
from get_Information_link import *
import re



def Translator(text, so, ta):
    translated = GoogleTranslator(source=so, target=ta).translate(text)
    return translated

def clean_text(text):
    try:
        return ' '.join(text.split()).strip()
    except:
        return text

def translate_text(text, source_lang='en', target_lang='fa'):
    """
    ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MyMemoryTranslator
    Ù…ØªÙ† Ø±Ø§ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
    text: str -> Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
    source_lang: str -> Ø²Ø¨Ø§Ù† Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ (default='en')
    target_lang: str -> Ø²Ø¨Ø§Ù† Ù…Ù‚ØµØ¯ (default='fa')
    """
    if not text or text.strip() == "":
        return ""  

    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†: Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¦ÛŒ
    cleaned_text = ' '.join(text.split()).strip()
    
    try:
        translator = MyMemoryTranslator(source=source_lang, target=target_lang)
        translated_text = translator.translate(cleaned_text)
        return translated_text
    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}"


def extract_related_part(question, previous_answer):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø®Ø´ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù¾Ø§Ø³Ø® - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
    """
    try:
        if not question or not previous_answer:
            return None
            
        # Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØŒ Ú©Ù„ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
        if len(previous_answer) < 100:
            return previous_answer
            
        stopwords = ["Ø§Ø³Øª", "Ù‡Ø³Øª", "Ø¢ÛŒØ§", "Ú†ÛŒØ³Øª", "Ú†ÛŒÙ‡", "Ø±Ø§", "Ø§Ø²", "Ø¯Ø±", "Ø¨Ù‡", "Ø¨Ø±Ø§ÛŒ", "Ù…ÛŒ", "Ú©Ù‡", "Ø§ÛŒÙ†", "Ø¢Ù†"]
        words = [w.strip("ØŸ!.") for w in question.split() if w not in stopwords and len(w) > 1]

        if not words:
            return None
            
        sentences = [s.strip() for s in previous_answer.split(".") if s.strip()]

        for key in words:
            for s in sentences:
                if key in s and len(s) > 10:  # ÙÙ‚Ø· Ø¬Ù…Ù„Ø§Øª Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±
                    return s.strip()

        # Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯ÛŒØŒ Ø§ÙˆÙ„ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
        for s in sentences:
            if len(s) > 10:
                return s.strip()

        return None
        
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± extract_related_part: {e}")
        return None

def save_to_file(question, answer, filename="Questions_database.txt"):
    if not answer:
        return
    with open(filename, "a", encoding="utf-8") as f:
        f.write(f"{answer}")
def load_memory_lines(filename="Questions_database.txt"):
    with open(filename, "r", encoding="utf-8") as f:
        return f.readlines()  # Ø®Ø±ÙˆØ¬ÛŒ Ù„ÛŒØ³Øª 

def similarity_fa(s1, s2):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([s1, s2])
    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]


def run_search(subject):
    # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯
    search_results = get_links(subject, num=3)
    if search_results:
        combined_content = []
        for i, result in enumerate(search_results, 1):
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø§Ø² Ù‡Ø± Ù„ÛŒÙ†Ú©
            content = extract_content(result["href"])        
            if content and not content.startswith("âŒ"):
                # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
                summary = content[:500] + "..." if len(content) > 500 else content
                combined_content.append(f"ğŸ“š {result['title']}\n{summary}")
    
        if combined_content:
            final_response = "ğŸ” Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ:\n\n" + "\n\n".join(combined_content[:2])
            return final_response
        else:
            return "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø§Ø² ØµÙØ­Ø§Øª ÙˆØ¨ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    else:
        return "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    

def needs_internet_search_en_v4(text):
    """
    Smarter English-only check for internet search necessity.
    - Uses substring matching
    - Handles multi-word keywords more robustly
    - Considers question, keywords, and sentence length
    """
    text_lower = text.lower().strip()
    
    
    is_question = text_lower.endswith('?') or any(text_lower.startswith(w) for w in ['what','how','who','which','when','where','can you'])
    
    search_keywords = [
        'method','tutorial','new','news','statistics','research','study','definition','meaning',
        'usage','advantages','disadvantages','history','python','artificial intelligence',
        'machine learning','data analysis','programming','code','algorithm','functions','class','object',
        'can you','about'
    ]
    
    words = text_lower.split()
    
    keyword_hits = 0
    for kw in search_keywords:
        kw_tokens = kw.split()
        for i in range(len(words) - len(kw_tokens) + 1):
            if words[i:i+len(kw_tokens)] == kw_tokens:
                keyword_hits += 1
                break  

    
    score = 0
    if is_question:
        score += 1
    score += keyword_hits * 2
    if len(words) > 5:
        score += 1
    
    return score >= 3

def clean_message(message):
    return re.sub(r'[^a-zA-Z\s]', '', message).lower()

# Ø¨Ø±Ø±Ø³ÛŒ Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ (Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø§Ø² Ø¹Ø¨Ø§Ø±Øª Ø¯Ø± Ù¾ÛŒØ§Ù… Ø¨Ø§Ø´Ø¯)
def phrase_in_message(phrase, message):
    words = phrase.split()
    return all(word in message for word in words)

request_verbs = [
    "help", "show", "give", "send", "tell", "explain",
    "provide", "teach", "find", "make", "walk", "assist",
    "guide", "support", "review", "clarify", "instruct",
    "demonstrate", "outline", "describe"
]

request_phrases = [
    "can you", "could you", "would you",
    "i need", "i want", "let me", "please", "could i", "would i"
]

request_indirect = [
    "it would be great if", "i would appreciate", "could someone",
    "would anyone", "i am looking for", "i am hoping", "i wonder if"
]

def is_request(message):
    message_clean = clean_message(message)
    score = 0
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø³ØªÙ‚ÛŒÙ…
    for phrase in request_phrases:
        if phrase_in_message(phrase, message_clean):
            score += 2
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙØ¹Ø§Ù„
    for verb in request_verbs:
        if verb in message_clean:
            score += 1
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ø¨Ø§Ø±Ø§Øª ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ…
    for indirect in request_indirect:
        if phrase_in_message(indirect, message_clean):
            score += 1.5
    
    return score > 2


def need_code(message):
    msg = message.lower()

    strong_patterns = [
        "write a program",
        "write code",
        "python code",
        "example code",
        "sample code",
        "show me code",
        "implement this",
        "how to code",
        "how do i implement",
        "how to implement",
        "write a function",
        "create a function"
    ]

    languages = ["python", "java", "c++", "c#", "javascript", "html", "css"]

    verbs = ["write", "generate", "create", "build", "implement", "simulate"]

    # Ø¹Ø¨Ø§Ø±Ø§Øª Ø®ÛŒÙ„ÛŒ ÙˆØ§Ø¶Ø­
    for p in strong_patterns:
        if p in msg:
            return True

    # Ø§Ú¯Ø± ÙØ¹Ù„ + Ø²Ø¨Ø§Ù† Ø¨ÛŒØ§ÛŒØ¯
    for v in verbs:
        for lang in languages:
            if v in msg and lang in msg:
                return True

    # Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡ program Ø¨ÛŒØ§ÛŒØ¯ + ÙØ¹Ù„
    if "program" in msg:
        for v in verbs:
            if v in msg:
                return True

    return False